---
title: "Project Analysis 2"
author: "Zhi Quan"
date: "2024-03-24"
output: html_document
---

# Set up 1
```{r}
library(randomForest)
library(glmnet)
library(e1071)
library(data.table)
library(dplyr)
df<- read.csv("final_hdb_resale_2023.csv",stringsAsFactors=TRUE)
selected_columns <- c("resale_price", "month", "area", "lat", "long", "floor_area_sqm",  "flat_model", "remaining_lease", "avg_storey", "no_of_rooms", "closestMall", "timeToMall", "mallsIn10km", "closestBeach", "timeToBeach", "beachesIn10km", "closestMRTStation", "timeToMRTStation", "MRTStationsIn10km", "closestPrimarySch", "timeToPrimarySch", "PrimarySchsIn10km", "below1000Pop", "below1500Pop", "below2000Pop", "below2500Pop", "below3000Pop", "below4000Pop", "below5000Pop", "below6000Pop", "below7000Pop", "below8000Pop", "below9000Pop", "below10000Pop", "below11000Pop", "below12000Pop", "above12000Pop")
new_df <- df[, which(names(df) %in% selected_columns)]
str(new_df)

# Splitting training and testing data
set.seed(444)
train <- sample(1:nrow(new_df),8000)
test <- -train
data.train <- new_df[train,]
data.test <- new_df[test,]
train.x <- model.matrix(resale_price~., data=data.train)[,-1]
train.y <- data.train$resale_price
test.x <- model.matrix(resale_price~., data=data.test)[,-1]
test.y <- data.test$resale_price
```


# Set up 2, Integer Encoding
```{r}
library(randomForest)
library(glmnet)
library(e1071)
library(data.table)
library(dplyr)
df<- read.csv("final_hdb_resale_2023.csv",stringsAsFactors=TRUE)
# Selecting 36 out of the 80 variables
selected_columns <- c("resale_price", "month", "area", "lat", "long", "floor_area_sqm",  "flat_model", "remaining_lease", "avg_storey", "no_of_rooms", "closestMall", "timeToMall", "mallsIn10km", "closestBeach", "timeToBeach", "beachesIn10km", "closestMRTStation", "timeToMRTStation", "MRTStationsIn10km", "closestPrimarySch", "timeToPrimarySch", "PrimarySchsIn10km", "below1000Pop", "below1500Pop", "below2000Pop", "below2500Pop", "below3000Pop", "below4000Pop", "below5000Pop", "below6000Pop", "below7000Pop", "below8000Pop", "below9000Pop", "below10000Pop", "below11000Pop", "below12000Pop", "above12000Pop")
new_df <- df[, which(names(df) %in% selected_columns)]
str(new_df)
# Converting the categorical variables that are factors to integers so that best subset selection can run with the leaps package
# Do one hot encoding for all categorical variables - to be used in clustering  
cols_to_encode <- new_df |>   select_if(~ is.factor(.)) |>   names()  
final_df <- new_df
final_df[,cols_to_encode] <- sapply(final_df[, cols_to_encode], function(x) as.integer(x))
str(final_df)

# Splitting training and testing data
set.seed(444)
train <- sample(1:nrow(final_df),8000)
test <- -train
data.train <- final_df[train,]
data.test <- final_df[test,]
train.x <- model.matrix(resale_price~., data=data.train)[,-1]
train.y <- data.train$resale_price
test.x <- model.matrix(resale_price~., data=data.test)[,-1]
test.y <- data.test$resale_price
```
# Set up 3, Target encoding
```{r}
library(randomForest)
library(glmnet)
library(e1071)
library(data.table)
library(dplyr)
df<- read.csv("final_hdb_resale_2023.csv",stringsAsFactors=TRUE)
# Selecting 36 out of the 80 variables
selected_columns <- c("resale_price", "month", "area", "lat", "long", "floor_area_sqm",  "flat_model", "remaining_lease", "avg_storey", "no_of_rooms", "closestMall", "timeToMall", "mallsIn10km", "closestBeach", "timeToBeach", "beachesIn10km", "closestMRTStation", "timeToMRTStation", "MRTStationsIn10km", "closestPrimarySch", "timeToPrimarySch", "PrimarySchsIn10km", "below1000Pop", "below1500Pop", "below2000Pop", "below2500Pop", "below3000Pop", "below4000Pop", "below5000Pop", "below6000Pop", "below7000Pop", "below8000Pop", "below9000Pop", "below10000Pop", "below11000Pop", "below12000Pop", "above12000Pop")
new_df <- df[, which(names(df) %in% selected_columns)]
str(new_df)
# Converting the categorical variables that are factors to integers so that best subset selection can run with the leaps package
# Do target encoding for all categorical variables - to be used in clustering
target_encode <- function(data, column, target) {
  # Check if column exists in data
  if (!(column %in% names(data))) {
    stop("Column not found in the data frame.")
  }
  
  # Check if target variable exists in data
  if (!(target %in% names(data))) {
    stop("Target variable not found in the data frame.")
  }
  
  # Check if column is a factor
  if (!is.factor(data[[column]])) {
    stop("Column is not a factor.")
  }
  
  # Check if column and target have the same length
  if (length(data[[column]]) != length(data[[target]])) {
    stop("Column and target variable must have the same length.")
  }
  
  # Calculate mean of target variable for each category in the specified column
  encoding <- tapply(data[[target]], data[[column]], mean)
  
  # Map the mean values to the corresponding categories and assign to a new column
  data[[paste0(column, "_encoded")]] <- encoding[data[[column]]]
  
  # Return the modified data frame with target-encoded column
  return(data)
} # function to do target encoding
cols_to_encode <- names(new_df)[sapply(new_df, is.factor)] # Select categorical columns for target encoding
for (col in cols_to_encode) {
  new_df <- target_encode(data=new_df, column=col,target="resale_price")
} # Perform target encoding
factor_cols <- sapply(new_df, is.factor)
new_df <- new_df[, !factor_cols]
str(new_df)

# Splitting training and testing data
set.seed(444)
train <- sample(1:nrow(new_df),8000)
test <- -train
data.train <- new_df[train,]
data.test <- new_df[test,]
train.x <- model.matrix(resale_price~., data=data.train)[,-1]
train.y <- data.train$resale_price
test.x <- model.matrix(resale_price~., data=data.test)[,-1]
test.y <- data.test$resale_price
```

# Best Subset Regression
```{r}
library(leaps)

# Best Subset Selection
bss1 <- regsubsets(resale_price~., data=data.train)
best_subset1 <- which.min(summary(bss1)$bic)
coef_best_subset1 <- coef(bss1,id=best_subset1)
print(coef_best_subset1)
test_mat <- model.matrix(resale_price~., data = data.test)
pred_bss1 <- test_mat[,names(coef_best_subset1)]%*%coef_best_subset1
rmse_bss1 <- sqrt(mean((test.y-pred_bss1)^2))
rmse_bss1
# Best subset selection RMSE with one hot encoding: 67754.2; with target encoding: 65766.01

# Forward Selection
bss3f <- regsubsets(resale_price~., data=data.train, method="forward")
best_subset3f <- which.min(summary(bss3f)$bic)
coef_best_subset3f <- coef(bss3f,id=best_subset3f)
print(coef_best_subset3f)
test_mat <- model.matrix(resale_price~., data = data.test)
pred_bss3f <- test_mat[,names(coef_best_subset3f)]%*%coef_best_subset3f
rmse_bss3f <- sqrt(mean((test.y-pred_bss3f)^2))
rmse_bss3f
# Forward selection RMSE with one hot encoding: 71359.92; with target encoding: 69802.32

# Backward Selection
bss3b <- regsubsets(resale_price~., data=data.train, method="backward")
best_subset3b <- which.min(summary(bss3f)$bic)
coef_best_subset3b <- coef(bss3b,id=best_subset3b)
print(coef_best_subset3b)
test_mat <- model.matrix(resale_price~., data = data.test)
pred_bss3b <- test_mat[,names(coef_best_subset3b)]%*%coef_best_subset3b
rmse_bss3b <- sqrt(mean((test.y-pred_bss3b)^2))
rmse_bss3b
# Backward selection RMSE with one hot encoding: 68720.47; with target encoding: 68317.05

# Both Forward and Backward selection
step_lm <- lm(resale_price~., data=data.train)
step <- step(step_lm)
summary(step)
pred_step <- predict(step, newdata=data.test)
mse_step <- mean((test.y-pred_step)^2)
rmse_step <- sqrt(mse_step)
rmse_step
# Both forward and backward RMSE with one hot encoding: 62824.98; with target encoding: 57423.42
```

# Bagging
```{r}
set.seed(444)
library(randomForest)
library(caret)
bag <- randomForest(resale_price~., data=data.train, importance=TRUE)
bag
pred_bag <- predict(bag, newdata=data.test)
mse_bag <- mean((test.y-pred_bag)^2)
rmse_bag <-sqrt(mse_bag)
rmse_bag
# Bagging RMSE with one hot encoding: 34061.47; with target encoding: 33756.24 (Rounding), 33904.42 (Default)
importance(bag)
```

# Random Forest
```{r}
library(randomForest)
library(caret)
set.seed(444)
rf <- randomForest(resale_price~., data=data.train, mtry=sqrt(ncol(data.train)-1), importance=TRUE)
rf
pred_rf <- predict(rf, newdata=data.test)
mse_rf <- mean((test.y-pred_rf)^2)
rmse_rf <- sqrt(mse_rf)
rmse_rf
# Random Forest RMSE with one hot encoding: 34189.06; with target encoding:33811.02
importance(rf)
```

# Support Vector Regression
```{r}
library(e1071)

# Linear (cost)
svr_l1 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='linear')
summary(svr_l1)
pred_svr_l1 <- predict(svr_l1, newdata=data.test)
mse_svr_l1 <- mean((test.y-pred_svr_l1)^2)
rmse_svr_l1 <- sqrt(mse_svr_l1)
rmse_svr_l1
# Linear1 RMSE: 58188.38


# Polynomial (cost, degree, gamma, epsilon, coeef.0)
svr_p1 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='polynomial')
summary(svr_p1)
pred_svr_p1 <- predict(svr_p1, newdata=data.test)
mse_svr_p1 <- mean((test.y-pred_svr_p1)^2)
rmse_svr_p1 <- sqrt(mse_svr_p1)
rmse_svr_p1
# Polynomial1 RMSE: 41464.67

# Radial (cost, epsilon, gamma)
svr_r1 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial')
summary(svr_r1)
# (1, 0.02777778,0.1)
pred_svr_r1 <- predict(svr_r1, newdata=data.test)
mse_svr_r1 <- mean((test.y-pred_svr_r1)^2)
rmse_svr_r1 <- sqrt(mse_svr_r1)
rmse_svr_r1
# Radial1 RMSE: 33212.64

# increasing gamma, keeping others constant
svr_r2 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=1, gamma=0.1, epsilon=0.1)
summary(svr_r2)
pred_svr_r2 <- predict(svr_r2, newdata=data.test)
mse_svr_r2 <- mean((test.y-pred_svr_r2)^2)
rmse_svr_r2 <- sqrt(mse_svr_r2)
rmse_svr_r2
# Radial2 RMSE: 37146.3

# decreasing gamma, keeping others constant
svr_r3 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=1, gamma=0.01, epsilon=0.1)
summary(svr_r3)
pred_svr_r3 <- predict(svr_r3, newdata=data.test)
mse_svr_r3 <- mean((test.y-pred_svr_r3)^2)
rmse_svr_r3 <- sqrt(mse_svr_r3)
rmse_svr_r3
# Radial3 RMSE: 36729.31

# increasing cost
svr_r4 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=2, gamma=0.02777778, epsilon=0.1)
summary(svr_r4)
pred_svr_r4 <- predict(svr_r4, newdata=data.test)
mse_svr_r4 <- mean((test.y-pred_svr_r4)^2)
rmse_svr_r4 <- sqrt(mse_svr_r4)
rmse_svr_r4
# Radial4 RMSE: 31675.6

# increasing cost
svr_r5 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=3, gamma=0.02777778, epsilon=0.1)
summary(svr_r5)
pred_svr_r5 <- predict(svr_r5, newdata=data.test)
mse_svr_r5 <- mean((test.y-pred_svr_r5)^2)
rmse_svr_r5 <- sqrt(mse_svr_r5)
rmse_svr_r5
# Radial5 RMSE: 31243.84

# increasing cost
svr_r6 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=4, gamma=0.02777778, epsilon=0.1)
summary(svr_r6)
pred_svr_r6 <- predict(svr_r6, newdata=data.test)
mse_svr_r6 <- mean((test.y-pred_svr_r6)^2)
rmse_svr_r6 <- sqrt(mse_svr_r6)
rmse_svr_r6
# Radial6 RMSE: 30999.79

# increasing cost
svr_r7 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=10, gamma=0.02777778, epsilon=0.1)
summary(svr_r7)
pred_svr_r7 <- predict(svr_r7, newdata=data.test)
mse_svr_r7 <- mean((test.y-pred_svr_r7)^2)
rmse_svr_r7 <- sqrt(mse_svr_r7)
rmse_svr_r7
# Radial7 RMSE: 30894.68

# increasing cost
svr_r8 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=100, gamma=0.02777778, epsilon=0.1)
summary(svr_r8)
pred_svr_r8 <- predict(svr_r8, newdata=data.test)
mse_svr_r8 <- mean((test.y-pred_svr_r8)^2)
rmse_svr_r8 <- sqrt(mse_svr_r8)
rmse_svr_r8
# Radial8 RMSE: 33584.71

# Calibrating cost
svr_r9 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=20, gamma=0.02777778, epsilon=0.1)
summary(svr_r9)
pred_svr_r9 <- predict(svr_r9, newdata=data.test)
mse_svr_r9 <- mean((test.y-pred_svr_r9)^2)
rmse_svr_r9 <- sqrt(mse_svr_r9)
rmse_svr_r9
# Radial9 RMSE: 31412.24

# Calibrating cost
svr_r10 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=15, gamma=0.02777778, epsilon=0.1)
summary(svr_r10)
pred_svr_r10 <- predict(svr_r10, newdata=data.test)
mse_svr_r10 <- mean((test.y-pred_svr_r10)^2)
rmse_svr_r10 <- sqrt(mse_svr_r10)
rmse_svr_r10
# Radial10 RMSE: 31206.15

# increasing epsilon
svr_r11 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=10, gamma=0.02777778, epsilon=0.2)
summary(svr_r11)
pred_svr_r11 <- predict(svr_r11, newdata=data.test)
mse_svr_r11 <- mean((test.y-pred_svr_r11)^2)
rmse_svr_r11 <- sqrt(mse_svr_r11)
rmse_svr_r11
# 32601.22

# decreasing epsilon
svr_r12 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=10, gamma=0.02777778, epsilon=0.05)
summary(svr_r12)
pred_svr_r12 <- predict(svr_r12, newdata=data.test)
mse_svr_r12 <- mean((test.y-pred_svr_r12)^2)
rmse_svr_r12 <- sqrt(mse_svr_r12)
rmse_svr_r12
# 30714.37

# decreasing epsilon
svr_r13 <- svm(resale_price~., data=data.train, type='eps-regression', kernel='radial', cost=10, gamma=0.02777778, epsilon=0.01)
summary(svr_r13)
pred_svr_r13 <- predict(svr_r13, newdata=data.test)
mse_svr_r13 <- mean((test.y-pred_svr_r13)^2)
rmse_svr_r13 <- sqrt(mse_svr_r13)
rmse_svr_r13
# 31005.55
```